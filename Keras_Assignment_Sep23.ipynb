{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Applied Data Science 2 - Keras Assignment - 2023A\n",
        "\n",
        "In this assignment you will be building a script to classify images of animals. The assignment is broken up into sections and you need to complete each section successively. The sections are:\n",
        "\n",
        "1. Data Processing\n",
        "2. Model Definition\n",
        "3. Model Training\n",
        "4. Model Evaluation\n",
        "\n",
        "In addition to this coding exercise, you will also need to write a 1-2 page report analysing and critically evaluating you models results."
      ],
      "metadata": {
        "id": "nQrpKRkEIBB5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eBNRzib8voMm"
      },
      "outputs": [],
      "source": [
        "# Enter your module imports here, some modules are already provided\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Init1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MGHk085OWCT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f193ba0f-97e7-435b-f852-4415a55794ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing\n",
        "\n"
      ],
      "metadata": {
        "id": "PN-7QYYKKCOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag DataProc\n",
        "\n",
        "label_dict = {'cat' : 0,\n",
        "              'dog' : 1,\n",
        "              'wild' : 2}\n",
        "\n",
        "# This function is provided to read in the image files from the folder on your\n",
        "# Google Drive\n",
        "def parse_image(filename):\n",
        "  parts = tf.strings.split(filename, os.sep)\n",
        "  label = parts[-2]\n",
        "  label = tf.strings.to_number(label)\n",
        "\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "  return image, label\n",
        "\n",
        "img_loc = \"/content/drive/MyDrive/Animals/\"\n",
        "\n",
        "train_list_ds = tf.data.Dataset.list_files(img_loc + \"train/*/*\")\n",
        "valid_list_ds = tf.data.Dataset.list_files(img_loc + \"val/*/*\")\n"
      ],
      "metadata": {
        "id": "9yWqczq8KGI6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a function called \"img_process\" converts the images to float32 datatype and resizes them to 64x64 pixels.**"
      ],
      "metadata": {
        "id": "q3hIMuVQM4AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex1a\n",
        "### Write a function called img_process, which takes in the image and label as\n",
        "### inputs, converts the data type of the image to tf.float32, resizes the\n",
        "### image to (64,64), and finally returns the image and labels.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def img_process(image, label, target_size=(64, 64)):\n",
        "    \"\"\"\n",
        "    Preprocesses an image by resizing and converting to float32.\n",
        "\n",
        "    Parameters:\n",
        "    - image (tf.Tensor): Input image tensor.\n",
        "    - label: The label associated with the image.\n",
        "    - target_size (tuple): Target size for resizing the image.\n",
        "\n",
        "    Returns:\n",
        "    - img_array (tf.Tensor): Preprocessed image tensor.\n",
        "    - label: The input label.\n",
        "    \"\"\"\n",
        "\n",
        "    # Resize the image to the target size\n",
        "    img_array = tf.image.resize(image, target_size)\n",
        "\n",
        "    # Convert to float32\n",
        "    img_array = tf.image.convert_image_dtype(img_array, tf.float32)\n",
        "\n",
        "    return img_array, label\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BxtGcsAWM3gs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the tf.data API, load in the training and validation data. Be mindful of efficient data processing good practice to minimise the time it takes to load the data.**"
      ],
      "metadata": {
        "id": "G51UPu0fNmNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex1b\n",
        "### Use the parse_image and img_process functions to construct the training and\n",
        "### validation datasets. You should utilise good practice in optimising the\n",
        "### dataset loading. Use a batch size of 128. Use techniques like caching and\n",
        "### prefetching to efficiently load the data.\n",
        "\n",
        "# Define parse_image function\n",
        "def parse_image(filename):\n",
        "    parts = tf.strings.split(filename, os.sep)\n",
        "    label = parts[-2]\n",
        "    label = tf.strings.to_number(label)\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.io.decode_jpeg(image)\n",
        "    return image, label\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = (\n",
        "    train_list_ds\n",
        "    .shuffle(buffer_size=1000)  # Shuffle the dataset\n",
        "    .map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # Parse the images and labels\n",
        "    .map(img_process, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # Preprocess the images\n",
        "    .batch(batch_size)\n",
        "    .cache()  # Cache the dataset for faster training\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)  # Prefetch the next batch while training\n",
        ")\n",
        "\n",
        "# Create validation dataset\n",
        "valid_ds = (\n",
        "    valid_list_ds\n",
        "    .map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # Parse the images and labels\n",
        "    .map(img_process, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # Preprocess the images\n",
        "    .batch(batch_size)\n",
        "    .cache()  # Cache the dataset for faster validation\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)  # Prefetch the next batch while validating\n",
        ")"
      ],
      "metadata": {
        "id": "VNUS8p98Ph5L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition\n",
        "\n",
        "**Using the Keras Functional API, create a convolutional neural network with the architecture show in the model summary below.**\n",
        "\n",
        "**A few important points to consider:**\n",
        "\n",
        "* Call the convolutional layers and the first dense layer should have ReLU activation functions. The output layer should have a SoftMax activation function.\n",
        "* Pay attention to the output shapes and the number of partmeters for each layer, as these give indications as to the correct settings for the number of filters, kernel size, stride length and padding.\n",
        "* Use the layer names provided in the summary in your model."
      ],
      "metadata": {
        "id": "86Uo36n_KGYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "# Model Summary\n",
        "\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " Input (InputLayer)          [(None, 64, 64, 3)]       0         \n",
        "                                                                 \n",
        " Conv0 (Conv2D)              (None, 32, 32, 16)        1216      \n",
        "                                                                 \n",
        " Conv1 (Conv2D)              (None, 32, 32, 32)        4640      \n",
        "                                                                 \n",
        " Conv2 (Conv2D)              (None, 32, 32, 32)        9248      \n",
        "                                                                 \n",
        " Pool1 (MaxPooling2D)        (None, 16, 16, 32)        0         \n",
        "                                                                 \n",
        " Conv3 (Conv2D)              (None, 16, 16, 64)        18496     \n",
        "                                                                 \n",
        " Conv4 (Conv2D)              (None, 16, 16, 64)        36928     \n",
        "                                                                 \n",
        " Pool2 (MaxPooling2D)        (None, 8, 8, 64)          0         \n",
        "                                                                 \n",
        " Conv5 (Conv2D)              (None, 8, 8, 128)         73856     \n",
        "                                                                 \n",
        " Conv6 (Conv2D)              (None, 8, 8, 128)         147584    \n",
        "                                                                 \n",
        " GlobalPool (GlobalAverageP  (None, 128)               0         \n",
        " ooling2D)                                                       \n",
        "                                                                 \n",
        " FC1 (Dense)                 (None, 512)               66048     \n",
        "                                                                 \n",
        " Output (Dense)              (None, 10)                5130      \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 363146 (1.39 MB)\n",
        "Trainable params: 363146 (1.39 MB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "_________________________________________________________________\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "hBSBfH-QP6M0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex2a\n",
        "### Define the model using the Keras Functional API. Use the summary above as a\n",
        "### guide for the model parameters. You will need to define the filters/units of\n",
        "### the layers correctly, as well as the kernel size, stride length and padding\n",
        "### of the convolutional layers.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (64, 64, 3)  # Assuming RGB images with size 64x64\n",
        "\n",
        "# Define input layer\n",
        "input_layer = Input(shape=input_shape, name=\"Input\")\n",
        "\n",
        "# Convolutional layers\n",
        "conv0 = Conv2D(16, (3, 3), activation='relu', padding='same', name=\"Conv0\")(input_layer)\n",
        "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', name=\"Conv1\")(conv0)\n",
        "conv2 = Conv2D(32, (3, 3), activation='relu', padding='same', name=\"Conv2\")(conv1)\n",
        "pool1 = MaxPooling2D((2, 2), name=\"Pool1\")(conv2)\n",
        "\n",
        "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same', name=\"Conv3\")(pool1)\n",
        "conv4 = Conv2D(64, (3, 3), activation='relu', padding='same', name=\"Conv4\")(conv3)\n",
        "pool2 = MaxPooling2D((2, 2), name=\"Pool2\")(conv4)\n",
        "\n",
        "conv5 = Conv2D(128, (3, 3), activation='relu', padding='same', name=\"Conv5\")(pool2)\n",
        "conv6 = Conv2D(128, (3, 3), activation='relu', padding='same', name=\"Conv6\")(conv5)\n",
        "global_pool = GlobalAveragePooling2D(name=\"GlobalPool\")(conv6)\n",
        "\n",
        "# Fully Connected layers\n",
        "fc1 = Dense(512, activation='relu', name=\"FC1\")(global_pool)\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(3, activation='softmax', name=\"Output\")(fc1)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "im9qipCiKJMK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex2b\n",
        "### Print the model summary and confirm is has the same architecture as the one\n",
        "### provided.\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "4ogeFQCqnz06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49c74e9-6368-40a5-8938-727a3e336474"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Input (InputLayer)          [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " Conv0 (Conv2D)              (None, 64, 64, 16)        448       \n",
            "                                                                 \n",
            " Conv1 (Conv2D)              (None, 64, 64, 32)        4640      \n",
            "                                                                 \n",
            " Conv2 (Conv2D)              (None, 64, 64, 32)        9248      \n",
            "                                                                 \n",
            " Pool1 (MaxPooling2D)        (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " Conv3 (Conv2D)              (None, 32, 32, 64)        18496     \n",
            "                                                                 \n",
            " Conv4 (Conv2D)              (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " Pool2 (MaxPooling2D)        (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " Conv5 (Conv2D)              (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " Conv6 (Conv2D)              (None, 16, 16, 128)       147584    \n",
            "                                                                 \n",
            " GlobalPool (GlobalAverageP  (None, 128)               0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " FC1 (Dense)                 (None, 512)               66048     \n",
            "                                                                 \n",
            " Output (Dense)              (None, 3)                 1539      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 358787 (1.37 MB)\n",
            "Trainable params: 358787 (1.37 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile the model using the Adam Optimizer with a learning rate of ```5e-5```, ```sparse categorical crossentropy``` loss function, and ```accuracy``` metric.**"
      ],
      "metadata": {
        "id": "7Q4qQnsuP-CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex2c\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qndXNKsyQl2G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model Training\n",
        "\n",
        "**Create a Model Checkpoint Callback that saves the weights of the best performing epoch, based on the validation accuracy.**"
      ],
      "metadata": {
        "id": "4G9gILWpKJXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex3a\n",
        "### Create a ModelCheckpoint callback to store the bext weights from the model,\n",
        "###Â based on the validation accuracy. Call this callback \"checkpoint_callback\"\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint file path\n",
        "checkpoint_filepath = '/content/checkpoint'\n",
        "\n",
        "# Create ModelCheckpoint callback\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',  # Monitor validation accuracy\n",
        "    mode='max',  # Save the model weights with the highest validation accuracy\n",
        "    save_best_only=True,  # Only save the best model\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "ulXjHaiZKLLZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Learning Rate Scheduler Callback that utilises the provided function to decrease the learning rate during training.**"
      ],
      "metadata": {
        "id": "9iUsL966RQMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex3b\n",
        "### Using the function provided, create a LearningRateScheduler callback, call\n",
        "### it \"lr_callback\"\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.01)\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Create LearningRateScheduler callback\n",
        "lr_callback = LearningRateScheduler(scheduler, verbose=1)"
      ],
      "metadata": {
        "id": "5jhC3C7qRX1B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model for 100 epochs, using the callbacks you made previously. Store the losses and metrics to use later.**"
      ],
      "metadata": {
        "id": "zQbqwZNjRYIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex3c\n",
        "### Train the model for 100 epochs, using the callbacks you have created. Store\n",
        "### the losses and metrics in a history object.\n",
        "# Train the model for 100 epochs with callbacks\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=100,\n",
        "    validation_data=valid_ds,\n",
        "    callbacks=[checkpoint_callback, lr_callback]\n",
        ")\n",
        "\n",
        "# Access training history\n",
        "training_loss = history.history['loss']\n",
        "training_accuracy = history.history['accuracy']\n",
        "\n",
        "# Access validation history\n",
        "validation_loss = history.history['val_loss']\n",
        "validation_accuracy = history.history['val_accuracy']"
      ],
      "metadata": {
        "id": "05qEIpORRfr8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "671005a1-0aaf-4cd4-872d-03d925426d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 4.999999873689376e-05.\n",
            "Epoch 1/100\n",
            " 11/115 [=>............................] - ETA: 36:49 - loss: 1.2756 - accuracy: 0.3438"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "**Create plots using the losses and metrics. In your report, discuss these results and critically evaluate the models performance.**"
      ],
      "metadata": {
        "id": "hkaP8sRwKLYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex4a\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Access training history\n",
        "training_loss = history.history['loss']\n",
        "training_accuracy = history.history['accuracy']\n",
        "\n",
        "print(training_loss)\n",
        "print('\\n')\n",
        "print(training_accuracy)\n",
        "print('\\n')\n",
        "# Access validation history\n",
        "validation_loss = history.history['val_loss']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "print(validation_loss)\n",
        "print('\\n')\n",
        "print(validation_accuracy)\n",
        "print('\\n')\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(training_loss, label='Training Loss')\n",
        "plt.plot(validation_loss, label='Validation Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(training_accuracy, label='Training Accuracy')\n",
        "plt.plot(validation_accuracy, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V-zTj4ZZKQ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the best weights from your model checkpoint, and create plots demonstrating the classification perfomnce for all three classes. Include these plots in your report, and critically evaluate on the performance of the model across the classes.**"
      ],
      "metadata": {
        "id": "be9li89iSP6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CodeGrade Tag Ex4b\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Load the best weights from the model checkpoint\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Assuming you have a test dataset (test_ds) defined\n",
        "# Replace with the actual path to your test dataset\n",
        "test_list_ds = tf.data.Dataset.list_files(\"/content/drive/MyDrive/Animals/val/*/*\")\n",
        "test_ds = test_list_ds.map(parse_image).map(img_process).batch(batch_size)\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "predictions = model.predict(test_ds)\n",
        "print('\\n')\n",
        "print(predictions)\n",
        "print('\\n')\n",
        "# Convert predictions to class labels\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "print('\\n')\n",
        "print(predicted_labels)\n",
        "print('\\n')\n",
        "# Extract true labels from the dataset\n",
        "true_labels = np.concatenate([label.numpy() for _, label in test_ds])\n",
        "print('\\n')\n",
        "print(true_labels)\n",
        "print('\\n')\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print('\\n')\n",
        "print(conf_matrix)\n",
        "print('\\n')\n",
        "# Plot confusion matrix\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=label_dict.keys(), yticklabels=label_dict.keys())\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Create classification report\n",
        "class_report = classification_report(true_labels, predicted_labels, target_names=label_dict.keys())\n",
        "print(\"Classification Report:\")\n",
        "print('\\n')\n",
        "print(class_report)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "x7BtdNrsSlai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}